---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: false
  eval: true
---

# ðŸŒ² Random Forest Challenge - The Power of Weak Learners

Navigate to the [Student Analysis Section](#student-analysis-section) to see the complete analysis and visualizations.

## Data and Methodology

We analyze the Ames Housing dataset, which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is ideal for our analysis because:

- **Anticipated Non-linear Relationships:** Real estate prices have complex, non-linear relationships between features (e.g., square footage in wealthy vs. poor zip codes affects price differently)
- **Mixed Data Types:** Contains both categorical (zipCode) and numerical variables
- **Real-world Complexity:** Captures the kind of messy, real-world data where ensemble methods excel

Since we anticipate non-linear relationships, random forests are well-suited to model the relationship between features and sale price.

```{r}
#| label: load-and-model-r
#| echo: false
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(scales))
suppressPackageStartupMessages(library(rpart))

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  # Convert zipCode to factor (categorical variable) - important for proper modeling
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

cat("Data prepared with zipCode as categorical variable\n")
cat("Number of unique zip codes:", length(unique(model_data$zipCode)), "\n")

# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1, mtry = 3, seed = 123)
rf_5 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5, mtry = 3, seed = 123)
rf_25 <- randomForest(SalePrice ~ ., data = train_data, ntree = 25, mtry = 3, seed = 123)
rf_100 <- randomForest(SalePrice ~ ., data = train_data, ntree = 100, mtry = 3, seed = 123)
rf_500 <- randomForest(SalePrice ~ ., data = train_data, ntree = 500, mtry = 3, seed = 123)
rf_1000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1000, mtry = 3, seed = 123)
rf_2000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 2000, mtry = 3, seed = 123)
rf_5000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5000, mtry = 3, seed = 123)
```

## Results: The Power of Ensemble Learning

Our analysis reveals a clear pattern: **more trees consistently improve performance**. Let's examine the results and understand why this happens.

### Performance Trends

```{r}
#| label: performance-comparison-r
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions and performance metrics for test data
predictions_1_test <- predict(rf_1, test_data)
predictions_5_test <- predict(rf_5, test_data)
predictions_25_test <- predict(rf_25, test_data)
predictions_100_test <- predict(rf_100, test_data)
predictions_500_test <- predict(rf_500, test_data)
predictions_1000_test <- predict(rf_1000, test_data)
predictions_2000_test <- predict(rf_2000, test_data)
predictions_5000_test <- predict(rf_5000, test_data)

# Calculate predictions for training data
predictions_1_train <- predict(rf_1, train_data)
predictions_5_train <- predict(rf_5, train_data)
predictions_25_train <- predict(rf_25, train_data)
predictions_100_train <- predict(rf_100, train_data)
predictions_500_train <- predict(rf_500, train_data)
predictions_1000_train <- predict(rf_1000, train_data)
predictions_2000_train <- predict(rf_2000, train_data)
predictions_5000_train <- predict(rf_5000, train_data)

# Calculate RMSE for test data
rmse_1_test <- sqrt(mean((test_data$SalePrice - predictions_1_test)^2))
rmse_5_test <- sqrt(mean((test_data$SalePrice - predictions_5_test)^2))
rmse_25_test <- sqrt(mean((test_data$SalePrice - predictions_25_test)^2))
rmse_100_test <- sqrt(mean((test_data$SalePrice - predictions_100_test)^2))
rmse_500_test <- sqrt(mean((test_data$SalePrice - predictions_500_test)^2))
rmse_1000_test <- sqrt(mean((test_data$SalePrice - predictions_1000_test)^2))
rmse_2000_test <- sqrt(mean((test_data$SalePrice - predictions_2000_test)^2))
rmse_5000_test <- sqrt(mean((test_data$SalePrice - predictions_5000_test)^2))

# Calculate RMSE for training data
rmse_1_train <- sqrt(mean((train_data$SalePrice - predictions_1_train)^2))
rmse_5_train <- sqrt(mean((train_data$SalePrice - predictions_5_train)^2))
rmse_25_train <- sqrt(mean((train_data$SalePrice - predictions_25_train)^2))
rmse_100_train <- sqrt(mean((train_data$SalePrice - predictions_100_train)^2))
rmse_500_train <- sqrt(mean((train_data$SalePrice - predictions_500_train)^2))
rmse_1000_train <- sqrt(mean((train_data$SalePrice - predictions_1000_train)^2))
rmse_2000_train <- sqrt(mean((train_data$SalePrice - predictions_2000_train)^2))
rmse_5000_train <- sqrt(mean((train_data$SalePrice - predictions_5000_train)^2))

# Calculate R-squared
r2_1 <- 1 - sum((test_data$SalePrice - predictions_1_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5 <- 1 - sum((test_data$SalePrice - predictions_5_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_25 <- 1 - sum((test_data$SalePrice - predictions_25_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_100 <- 1 - sum((test_data$SalePrice - predictions_100_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_500 <- 1 - sum((test_data$SalePrice - predictions_500_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_1000 <- 1 - sum((test_data$SalePrice - predictions_1000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_2000 <- 1 - sum((test_data$SalePrice - predictions_2000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5000 <- 1 - sum((test_data$SalePrice - predictions_5000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create performance comparison
performance_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

print(performance_df)
```

## Student Analysis Section: {#student-analysis-section}

### 1. The Power of More Trees Visualization

```{r}
#| label: power-of-trees-visualization
#| echo: false
#| fig-width: 12
#| fig-height: 8
#| message: false
#| warning: false

# Create the visualization data
viz_data <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

# Create RMSE plot
rmse_plot <- ggplot(viz_data, aes(x = Trees)) +
  geom_line(aes(y = RMSE_Test, color = "Test Data"), size = 1.2) +
  geom_line(aes(y = RMSE_Train, color = "Training Data"), size = 1.2) +
  geom_point(aes(y = RMSE_Test, color = "Test Data"), size = 2.5) +
  geom_point(aes(y = RMSE_Train, color = "Training Data"), size = 2.5) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_color_manual(values = c("Test Data" = "#E31A1C", "Training Data" = "#1F78B4")) +
  labs(
    title = "RMSE vs Number of Trees",
    subtitle = "Lower RMSE indicates better predictive performance",
    x = "Number of Trees (Log Scale)",
    y = "Root Mean Square Error (RMSE)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 12),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )

# Create R-squared plot
r2_plot <- ggplot(viz_data, aes(x = Trees, y = R_squared)) +
  geom_line(color = "#2E8B57", size = 1.2) +
  geom_point(color = "#2E8B57", size = 2.5) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "R-squared vs Number of Trees",
    subtitle = "Higher R-squared indicates better model fit",
    x = "Number of Trees (Log Scale)",
    y = "R-squared (Variance Explained)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 12),
    panel.grid.minor = element_blank()
  )

# Combine plots
grid.arrange(rmse_plot, r2_plot, ncol = 1)
```

### Analysis: The Power of More Trees

The biggest jump in performance happens early on, especially when going from just one tree to about 25 trees. In that range, RMSE drops sharply from roughly $47,000 to around $35,000, and R-squared climbs from about 0.65 to 0.82. This shows how quickly model accuracy improves once you start combining multiple trees instead of relying on a single decision tree. After around 100 trees, though, the curve starts to flattenâ€”each additional batch of trees only gives small, incremental gains. Between 100 and 500 trees, RMSE improves by only about $1,000, and beyond that, the difference becomes almost negligible. Overall, the pattern highlights diminishing returns: most of the improvement happens in the first few dozen trees, while more trees mainly fine-tune the model. For this dataset, somewhere between 100 and 500 trees seems to strike the best balance between accuracy and efficiency.

### 2. Overfitting Visualization and Analysis

```{r}
#| label: overfitting-analysis
#| echo: false
#| fig-width: 14
#| fig-height: 8
#| message: false
#| warning: false

# Create decision trees with different max depths
depths <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
dt_train_rmse <- numeric(length(depths))
dt_test_rmse <- numeric(length(depths))

for (i in seq_along(depths)) {
  # Build decision tree with specific max depth
  dt_model <- rpart(SalePrice ~ ., data = train_data, 
                    control = rpart.control(maxdepth = depths[i]))
  
  # Calculate predictions
  dt_train_pred <- predict(dt_model, train_data)
  dt_test_pred <- predict(dt_model, test_data)
  
  # Calculate RMSE
  dt_train_rmse[i] <- sqrt(mean((train_data$SalePrice - dt_train_pred)^2))
  dt_test_rmse[i] <- sqrt(mean((test_data$SalePrice - dt_test_pred)^2))
}

# Create data frames for plotting
dt_data <- data.frame(
  Complexity = depths,
  Train_RMSE = dt_train_rmse,
  Test_RMSE = dt_test_rmse
)

# Use existing random forest data (from previous analysis)
rf_data <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  Train_RMSE = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, 
                 rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  Test_RMSE = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, 
                rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test)
)

# Set consistent y-axis limits
y_min <- min(c(dt_data$Train_RMSE, dt_data$Test_RMSE, rf_data$Train_RMSE, rf_data$Test_RMSE))
y_max <- max(c(dt_data$Train_RMSE, dt_data$Test_RMSE, rf_data$Train_RMSE, rf_data$Test_RMSE))

# Create decision tree plot
dt_plot <- ggplot(dt_data, aes(x = Complexity)) +
  geom_line(aes(y = Train_RMSE, color = "Training Data"), size = 1.2) +
  geom_line(aes(y = Test_RMSE, color = "Test Data"), size = 1.2) +
  geom_point(aes(y = Train_RMSE, color = "Training Data"), size = 2.5) +
  geom_point(aes(y = Test_RMSE, color = "Test Data"), size = 2.5) +
  scale_color_manual(values = c("Test Data" = "#E31A1C", "Training Data" = "#1F78B4")) +
  scale_y_continuous(limits = c(y_min, y_max)) +
  labs(
    title = "Decision Trees: Overfitting with Complexity",
    subtitle = "Training performance improves while test performance degrades",
    x = "Max Depth (Tree Complexity)",
    y = "Root Mean Square Error (RMSE)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 12),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )

# Create random forest plot
rf_plot <- ggplot(rf_data, aes(x = Trees)) +
  geom_line(aes(y = Train_RMSE, color = "Training Data"), size = 1.2) +
  geom_line(aes(y = Test_RMSE, color = "Test Data"), size = 1.2) +
  geom_point(aes(y = Train_RMSE, color = "Training Data"), size = 2.5) +
  geom_point(aes(y = Test_RMSE, color = "Test Data"), size = 2.5) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_color_manual(values = c("Test Data" = "#E31A1C", "Training Data" = "#1F78B4")) +
  scale_y_continuous(limits = c(y_min, y_max)) +
  labs(
    title = "Random Forests: No Overfitting with More Trees",
    subtitle = "Both training and test performance improve together",
    x = "Number of Trees (Log Scale)",
    y = "Root Mean Square Error (RMSE)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 12),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )

# Combine plots side by side
grid.arrange(dt_plot, rf_plot, ncol = 2)
```

### Analysis: Overfitting in Decision Trees vs Random Forests

The side-by-side graphs clearly show how differently decision trees and random forests handle model complexity. As the depth of a single decision tree increases, training RMSE keeps improving, but test RMSE eventually starts getting worse â€” a textbook sign of overfitting. Around depth 4, the test error stops improving and begins to climb, meaning the model is memorizing the training data instead of learning patterns that generalize. By depth 10, the gap between training and test RMSE is large, showing that the model has become overly specialized. In contrast, the random forest graph shows consistent improvement for both training and test RMSE as more trees are added. The gap between them stays small, even with thousands of trees, which means the model stays stable and generalizes well. This happens because random forests blend results from many slightly different trees â€” each built with random samples and features â€” which reduces bias, smooths noise, and prevents overfitting. Overall, this comparison makes it easy to see why random forests are more reliable and practical for real-world prediction tasks.

### 3. Linear Regression vs Random Forest Comparison

```{r}
#| label: linear-regression-comparison
#| echo: false
#| message: false
#| warning: false

# Build linear regression model
lm_model <- lm(SalePrice ~ ., data = train_data)
lm_train_pred <- predict(lm_model, train_data)
lm_test_pred <- predict(lm_model, test_data)

# Calculate linear regression RMSE
lm_train_rmse <- sqrt(mean((train_data$SalePrice - lm_train_pred)^2))
lm_test_rmse <- sqrt(mean((test_data$SalePrice - lm_test_pred)^2))

# Create comparison table
comparison_data <- data.frame(
  Model = c("Linear Regression", "Random Forest (1 tree)", "Random Forest (100 trees)", "Random Forest (1000 trees)"),
  Train_RMSE = c(lm_train_rmse, rmse_1_train, rmse_100_train, rmse_1000_train),
  Test_RMSE = c(lm_test_rmse, rmse_1_test, rmse_100_test, rmse_1000_test),
  stringsAsFactors = FALSE
)

# Calculate percentage improvements over linear regression
comparison_data$Test_Improvement <- round(((lm_test_rmse - comparison_data$Test_RMSE) / lm_test_rmse) * 100, 1)
comparison_data$Train_Improvement <- round(((lm_train_rmse - comparison_data$Train_RMSE) / lm_train_rmse) * 100, 1)

# Format the table for display
comparison_table <- comparison_data %>%
  mutate(
    Train_RMSE = paste0("$", format(round(Train_RMSE), big.mark = ",")),
    Test_RMSE = paste0("$", format(round(Test_RMSE), big.mark = ",")),
    Test_Improvement = paste0(Test_Improvement, "%"),
    Train_Improvement = paste0(Train_Improvement, "%")
  ) %>%
  select(Model, Train_RMSE, Test_RMSE, Test_Improvement, Train_Improvement)

# Display the comparison table
knitr::kable(comparison_table, 
             col.names = c("Model", "Training RMSE", "Test RMSE", "Test Improvement", "Training Improvement"),
             caption = "Linear Regression vs Random Forest Performance Comparison",
             align = c("l", "r", "r", "r", "r"))
```

### Analysis: Linear Regression vs Random Forest Trade-offs

This comparison highlights when it makes sense to use linear regression versus random forests. Moving from one tree to 100 trees cuts test error by about 25%, and compared to linear regression, the 100-tree model performs roughly 30% better â€” a clear sign that non-linear patterns in the housing data matter. Random forests handle these complexities automatically, offering much stronger predictive power, while linear regression remains faster and easier to interpret. For this task, the random forestâ€™s major boost in accuracy outweighs its added complexity, making it the more practical choice for prediction, whereas linear regression is best reserved for cases where interpretability is the top priority.